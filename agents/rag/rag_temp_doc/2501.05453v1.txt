An Empirical Study of Autoregressive Pre-training
from Videos
Jathushan Rajasegaran1,2, Ilija Radosavovic2, Rahul Ravishankar2, Yossi Gandelsman1,2,
Christoph Feichtenhofer1, Jitendra Malik1,2
1Meta FAIR,2UC Berkeley
We empirically study autoregressive pre-training from videos. To perform our study, we construct a
series of autoregressive video models, calledToto. We treat videos as sequences of visual tokens and
train transformer models to autoregressively predict future tokens. Our models are pre-trained on a
diverse dataset of videos and images comprising over 1 trillion visual tokens. We explore different
architectural, training, and inference design choices. We evaluate the learned visual representations
on a range of downstream tasks including image recognition, video classification, object tracking, and
robotics. Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training
leads to competitive performance across all benchmarks. Finally, we find that scaling our video models
results in similar scaling curves to those seen in language models, albeit with a different rate.
Website: https://brjathu.github.io/toto
1 Introduction
In a paper published in 1951, Shannon, having just published the foundational papers of information theory,
proposed a “guessing game” ofnext word predictionto estimate the entropy of English (Shannon, 1951). Nearly
70 years later, training a high-capacity transformer network (Vaswani et al., 2017) on this task, provided
the generative pre-training backbone for Large Language Models (Radford et al., 2018; Devlin et al., 2019;
Radford et al., 2019; Brown et al., 2020).
Less well known is the fact that in 1954, Fred Attneave (Attneave, 1954) proposed an analog of Shannon’s
task for images. To quote “We may divide the picture into arbitrarily small elements which we “transmit” to a
subject (S) in a cumulative sequence, having them guess at the color of each successive element until they are
correct. This method of analysis resembles the scanning process used in television and facsimile systems and
accomplishes the like purpose of transforming two spatial dimensions into a single sequence in time”.
While Attneave was concerned with images, in the context of 2024, we have to note that the “Big Visual Data”
is in videos. While there are concerns that most of the text available on the Internet has already been used by
the language models, in video we just started on the journey of Big Data exploitation. Despite the successes
of autoregressive language and image models, their effectiveness for video modeling remains underexplored.
In this paper, we empirically study autoregressive pre-training from videos. To perform our empirical study,
we construct a family of autoregressive video models which we callToto. We treat videos as sequences of
visual tokens and train a causal transformer models on next-token prediction task. We use causal transformer
model with LLaMa (Touvron et al., 2023) architecture. We use dVAE (Ramesh et al., 2021) to tokenize
frames into discrete tokens. Treating videos as sequences of tokens enables us to jointly train on videos and
images using a unified format. We construct a diverse dataset of videos and images comprising over 1 trillion
visual tokens. Our models are first pre-trained on this data and then evaluated on downstream tasks. We
extract visual representations using attention pooling from relevant layers of the model.
We evaluate our models on various downstream tasks from image and video recognition, video forecasting,
semi-supervised tracking, object permanence and robotics tasks in both simulation and real-world. We
consider different design choices such as tokenizers including dVAE (Ramesh et al., 2021), VQGAN (Rombach
et al., 2022) and continuous patch-normalized (He et al., 2022) tokens. We also consider different architectures
such as LLaMA (Touvron et al., 2023), GPT2 (Radford et al., 2019) and Mamaba Gu & Dao (2023). Finally
we study the compute optimal scaling behaviors of autoregressive video models.
1
arXiv:2501.05453v1  [cs.CV]  9 Jan 2025Tokenizer
Tokens
Pre-training
Parrot
Run..TasksData
ImageNetEgo Videos
Large Scale Videos
Transformer
Frames
Figure 1 Overall Framework.Starting with images and video frames from a collection of datasets, we tokenize each
frame/image into discrete visual tokens independently. We pre-train the transformer by predicting the next visual
tokens, with a context length of 4K tokens of images or video frames. Once trained, we take the intermediate
representations and evaluate them on various tasks.
We find that, for tokenization autoregressive models based on discrete and continuous patch-normalized (He
et al., 2022) tokens perform similarly on ImageNet classification task. For efficient pre-training, starting with
lower resolution and fine-tuning at higher resolution gives better performance and RoPE (Su et al., 2024)
helps with adopting to higher resolution. For measuring the representation quality in decoder-only models,
due to skewed nature of the receptive field we use attention pooling over average pooling. We find that in
decoder-only models, for all tasks and models sizes the middle layer gives the best performance. Finally, we
study the scaling behaviors of autoregressive vision models, which scales with more compute but still at a
slower rate compared to large language models.
2 Related work
Representation Learning for Vision:Over the years self-supervised pre-training has proven to be effective in
many areas including language, vision, and robotics. Wu et al. (2018) and SimCLR (Chen et al., 2020b)
showed that instance discrimination training can learn strong discriminative features. MoCo (He et al., 2020)
and DINO (Caron et al., 2021) showed the effectiveness of strong visual representations on various downstream
tasks. Differently, BEiT (Bao et al., 2021) and MAE (He et al., 2022) used masked autoencoding for learning
image representations. ST-MAE (Feichtenhofer et al., 2022)and VideoMAE (Wang et al., 2023a) extended this
masked modeling approach to videos, by masking a large amount of tokens during pre-training and predict
the masked tokens with a light-weight decoder.
Autoregressive Modeling of Vision:Generative autoregressive pre-training learns to directly model the data
distribution. Inlanguagemodels, generativepre-traininghasbecomethestandardfortraininglargemodels. For
autoregressive pre-training in vision, rCNN (Ranzato et al., 2014), PixelCNN (Van den Oord et al., 2016) and
PixelRNN(VanDenOordetal.,2016)proposedgeneratingpixelsonebyoneusingconvolutionandbidirectional
LSTMs. With the introduction of the transformers (Vaswani et al., 2017), ImageTransformers (Parmar et al.,
2018) showed generating pixels with causal local attention performs better than previous CNN and RNN-based
methods. While all of these methods focused on the generation quality of the pixels, iGPT (Chen et al.,
2020a) showed that generative pre-training is also a good way to learn strong visual representations for
recognition tasks. Henighan et al. (2020) showed scaling behaviors of autoregressive image and video models.
AIM (El-Nouby et al., 2024) on the other hand uses patch embedding rather than any pre-trained models
for tokenization, however, it trains on Data Filtering Networks (Fang et al., 2023) with clip filtered data.
Compared to these works, we do not use any supervision during our pre-training and utilizes image and
videos jointly. VisionMamba (Zhu et al., 2024) also showed how to utilize sequence models with bidirectional
state-space modeling for supervised vision tasks. Weissenborn et al. (2019) showed autoregressive video
generation for promotable video generations.
Evaluation of Vision Representations:Most video pre-training models are evaluated on semantic tasks like
ImageNet (Deng et al., 2009) and Kinetics (Kay et al., 2017). Additionally to the standard evaluation, we
evaluate our models on semi-supervised tracking task on DAVIS (Pont-Tuset et al., 2017), action forecasting
on Ego4D (Grauman et al., 2022), object permanence on CATER (Girdhar & Ramanan, 2019) and on robot
manipulation tasksn simulation (Xiao et al., 2022) and in the real world Radosavovic et al. (2023).
23 Approach
We train a casual transformer model to predict the next patch tokens in images and videos. This is akin to
the next token prediction in large language models. From the vast collection of images and videos, every patch
is tokenized into a discrete token, and the transformer is trained to predict the next token, using raster scan
ordering. We pre-train our models on over one trillion tokens. Finally, we evaluate the learned representations
of these models on various downstream tasks including image classification, action classification, action
anticipation, video tracking, object permanence, and robotic manipulation tasks. We also study the scaling
behaviors of our models for compute optimal training.
3.1 Pre-training
Figure 2 Training Loss Curves:We show
the training loss curves for base, large,
and 1b models trained with tokens from
dVAE (Ramesh et al., 2021) with a vo-
cabulary size of 8k and context length
of 4k tokens (equivalent to 16 images or
video frames).
Given a large collection of images and videos, we tokenize all of
them into a 1D sequence using raster scan ordering. This produces
a dataset of tokens,{xj
1, xj
2, xj
3, ..., xj
n} where j is the sample either
from a video or an image andn is the number of tokens in an image
or a video. We model the densityp(x) as :
p(xj) =
nY
i=1
p(xj
i |xj
i−1, xj
i−2, ..., xj
1, θ) (1)
Here, θ is the model parameters, which can be optimized by min-
imizing the negative log-likelihood loss:
Lpre-train = E
xj ∼X
−log p(xj). (2)
Using this loss, we pre-train our models at different sizes on over one
visual trillion tokens. These tokens are generated from images and
video. Figure 2 shows the training loss of 3 differently sized models
with 120M, 280m and 1.1b parameters.
3.2 Architecture
Our model is a transformer (Vaswani et al., 2017) with causal attention. We apply recent advancements in
language modeling such as pre-norm using RMSNorm (Zhang & Sennrich, 2019), SwiGLU activation (Shazeer,
2020), and RoPE positional embeddings (Su et al., 2024), following LLaMa (Touvron et al., 2023).
For a model withL layers, we defineHl to be the intermediate representations after layerl, 0 ≤ l ≤ L. The
intermediate representations after layerl + 1, Hl+1, defined to be:
bHl+1 = Hl + MHSA(RMS-norm(Hl)) (3)
Hl+1 = bHl+1 + MLP(RMS-norm( bHl+1)), (4)
Where MHSA is a multi-head self attention layer,MLP is a multi-layer perceptron with SwiGLU activations.
We train our models for the next token prediction task at different scales (base, large and 1b models). For
more architecture details see Table 1. We train all these models with a batch size of 1M tokens. We use
AdamW (Loshchilov & Hutter, 2017) with a maximum learning rate of3e−4, andβ1 = 0.9, β2 = 0.95. We
decay the learning rate with a cosine schedule, after 2000 warm-up steps (Touvron et al., 2023).
Model Params Dimension Heads Layers
base 120m 768 12 12
large 280m 1024 16 16
1b 1.1b 2048 16 22
Table 1 Model Architecture: We pre-train models at different scales, only on visual tokens from images and videos.
3Datasets Instances Tokens Hours
ImageNet 13.9M 3.6B -
Kinetics-600 0.53M 41.3B 1496
Ego4D 52.1K 103B 3750
HowTo100m 1.172M 2560B 92627
Table 2 Pre-training Dataset: We use both image datasets (Imagenet (Russakovsky et al., 2015)) and video datasets
(Kinetics600 (Carreira et al., 2019), Ego4D (Grauman et al., 2022), HowTo100m (Miech et al., 2019)) with different
mixing ratios during the pre-training of our models. The whole training data contains about 100,000 hours of videos.
3.3 Dataset
To train our model, we compile a large dataset from a number of different sources. Table 2 shows the total
number of images and videos used for training data, the total number of tokens, as well as the number of
hours of videos in each dataset. Together these datasets contain over 100,000 hours of video data and about
2.5 trillion visual tokens. During training, each mini-batch is sampled at different ratios of datasets. Each
batch approximately contains 20% of ImageNet images, 10% of Ego4D videos, 10% of Kinetics videos, and
60% of HowTo100m videos. Our full training utilized about 1 trillion tokens.
3.4 Tokenization
We use dVAE tokenizer with a vocabulary of 8k tokens, from Dall-E (Ramesh et al., 2021) as our tokenizer.
Using an image-based tokenizer allows training on both images and videos and testing on respective downstream
tasks. While VQGAN (Esser et al., 2020) tokenizers provide sharper images, these models were trained with
perceptual loss (Larsen et al., 2016; Johnson et al., 2016), thus indirectly ingesting ImageNet label information
via VGG-net (Simonyan & Zisserman, 2014).
All raw pixel frames or images are tokenized into 256 discrete tokens. We take a video and resize it such that
its shortest size isR pixels, and then take a random crop ofR × R × T, and sample every 4 frames whereT
is the number of frames. We use dVAE (Ramesh et al., 2021) with the vocabulary of 8k entries to tokenize
every frame independently. For dVAE we setR = 128, to get16 × 16 discrete tokens. Once every frame is
mapped into a set of discrete tokens we haveT × 256 tokens per each video. We pre-train all the models with
T = 16, thus all the models are per-trained for a context length of 4096 tokens.
When training with images and videos, 16 video frames are sampled to create 4k tokens. For images, we
randomly sample 16 images and create a sequence of 16 image frames to generate 4k tokens. Finally, we add
start and end tokens for each sequence, for videos we use[1] as the start token, and for images we use[3] as
the start token, and all sequences have an end token of[2].
3.5 Downstream Transfer
The idea of large pre-trained models is that they were trained at a large compute scale, and then these models
can be easily used for various downstream tasks without requiring task-specific design or lots of computing for
transfer. The learned representations are general enough to transfer to various tasks. We evaluate our models
on the intermediate features with linear and attention probing (Lee et al., 2019).
For linear probing the model, we apply global average pooling (Lin et al., 2013) over the tokens from different
layers to get the intermediate representation. We train a linear layer on top of this representation on the
downstream task. MAE (He et al., 2022) or DINO (Caron et al., 2021) have a uniform structure when it
comes to which token attends to which tokens, however in autoregressive sequence modeling later tokens
attent to more tokens than the tokens at the beginning. Due to this skewed nature, equally weighting all the
tokens affects the downstream performance. Attention pooling is an alternative to average pooling that allows
to dynamically weight the tokens, ideally giving more weight to tokens that see more tokens. This requires
learning Wk and Wv matrices and a query tokenq. The query token cross-attends to the intermediate tokens
and combines them into a single vector. While this function is not linear anymore, it has been shown to learn
better representations in recent works (El-Nouby et al., 2024).
44 Experiments
We evaluate our pre-trained models on various downstream tasks such as ImageNet classification, Kinetics
action recognition, Ego4D action anticipation, Semi-Supervised tracking, and Robotic manipulation tasks.
First, we discuss various design choices for pre-training and evaluation strategies for our method. All the
models for studying the design choices arelarge models trained for 400 epochs on the ImageNet-1k dataset.
4.1 Design Choices
Tokenizer: The are various options available for tokenizing an image or a video. We could use discrete
tokenizers such as dVAE, and VQGAN, or simple patch-based continuous tokenization. To study the behavior
of various tokenizers we pre-train aToto-large model on ImageNet for 400 epochs. Using linear probing at an
optimal intermediate layer, we evaluate the accuracy of the models on ImageNet classification task.
Table 3 shows linear probing accuracy when trained with various tokenizers. VQGAN (Esser et al., 2020) and
dVAE (Ramesh et al., 2021) perform similarly with the same resolutions. However, VQGAN is contaminated
with ImageNet label information via perceptual loss. In addition to that, as shown in Figure 3, dVAE tokens
have full coverage compared to VQGAN tokens on their 1-gram distributions. Please see the supplementary
material for more details. Regressing normalized-patch targets from patch embeddings performs slightly worse
than classifying discrete tokens as targets. Additionally, discrete tokens as targets and patch embeddings as
inputs perform poorly compared to other methods at the given input-output resolutions. Overall, Table 3
shows that various ways of tokenization havelittle effecton ImageNet linear probing accuracy.
Figure 3 1-gram Distribution of Various Tokens:This Figure shows the distribution of 1-gram tokens of various tokenizers
(dVAE (Ramesh et al., 2021), VQGAN-1k, VQGAN-16k (Esser et al., 2020)) on Imagenet validation set. Note that,
dVAE has almost full convergence of the tokens while VQGAN has less than 50% coverage of the tokens.
Input-Target Tokens Vocabulary Top1
VQGAN-VQGAN 16x16 16k 61.3
VQGAN-VQGAN 16x16 1k 61.1
dVAE-dVAE 32x32 8k 61.2
dVAE-dVAE 16x16 8k 53.2
patch-patch 16x16 - 60.6
patch-dVAE 16x16 8k 58.5
Table 3 ImageNet Linear Probing Accuracy with Various Tokenizers:We compare discrete (dVAE, VQGAN) and patch
embedding as input and target for pre-training our models. ImageNet top-1 accuracies are computed by linear probing
at the 9th layer of thelarge model.
How to Probe:As discussed in Section 3.5 we probe the pre-trained models at the same layer with attention
pooling and average pooling, followed by linear layer. Table 5 shows attention pooling performs 7.9% higher
than average pooling on the ImageNet classification task. For attention pooling, we keep the embedding
dimension the same as the intermediate feature dimensions.
5Method Compute Top1
dVAE/16 1.42 × 1017 53.2
dVAE/32 5.68 × 1017 61.2
dVAE/16→32 2.13 × 1017 63.2
dVAE/16→32† 2.13 × 1017 64.4
Table 4 Token Resolution:While the performance is lower for a
low-resolution model, when finetuned for next-patch prediction at
a higher resolution, its performance surpasses the full-resolution
pre-trained model. † Base values of the RoPE is 50,000.
Method Tokens Pooling Top1
dVAE 16x16 Average 53.2
dVAE 16x16 Attention 61.1
Table 5 Attention vs Average Pooling: When
probed at the same layers, attention pooling
performs much better than average pooling of
intermediate tokens.
Resolution: When training with dVAE tokens, a 256x256 image results in 1024 tokens, this is four times more
number of tokens compared to patch embeddings or VQGAN tokens. If we reduce the number of tokens to
256, then the effective image resolution becomes 128x128. Table 4 shows a clear drop in performance when
pre-training the model at 128x128 resolution. However, due to the use of relative positional embeddings
(RoPE (Su et al., 2024)), we can easily finetune the 128x128 (or 16x16 token equivalent) model for higher
resolution. Surprisingly, this does better than pre-training at 256x256 resolution and requires only one epoch of
finetuning. Not only does this improve the performance, but the pre-training also becomes cheaper compared
to full-resolution pre-training.†Additionally, Fine-tuning with higher base values of the RoPE embeddings
(50,000) leads to better accuracy.
Architecture: We train various language models from GPT2 (Radford et al., 2019) with absolute sine-cosine
positional embeddings, and non-transformer based model Mamba (Gu & Dao, 2023) only using dVAE tokens.
We mimicked the GPT2 architecture and do architecture comparisons. We compare these models with
LLaMA (Touvron et al., 2023). We evaluate linear probing performance at each layer of these models and
report the best performance in Table 6.
Probing Layer: When probing the pre-trained models, especially the decoder-only model best performance is
observed at the middle layers. This behavior is first observed in iGPT (Chen et al., 2020a). Figure 4 shows
the peak performance on recognition occurs at about 50% of the depth of the model. This behavior holds
across all model sizes. While in MAE (He et al., 2022) and BEiT (Bao et al., 2021) encoder-decoder models,
due to the uneven nature of the encoder and decoder, the best features are observed at the top of the encoder
layers. However, on decoder-only models with uniformly distributed layers, the last layers perform worse on
recognition tasks, mainly because these layers are trained to reconstruct the input. More probing results with
various tokenizers, resolutions, and probing methods are shown in the supplementary material.
Model Params Top1
GPT2 Radford et al. (2019) 280 m 48.5
Mamba Gu & Dao (2023) 290 m 40.7
LLaMA Touvron et al. (2023) 280 m 53.2
Table 6 Architecture: We compare sequence mod-
eling architectures LLaMA Touvron et al. (2023),
GPT2 Radford et al. (2019), and non-transformer
models, Mamba Gu & Dao (2023) on ImageNet linear
probing task.
Figure 4 Probing at Different Layers:We show the attention-
probing performance at each layer of our three models. Peak
performance is observed at around 50% depth of the models.
64.2 Image Recognition
To measure the representation quality of our pre-trained models, we evaluate our models on ImageNet-1k (Deng
et al., 2009) classification. We apply a probe at each layer of the model, with attention pooling, and choose
the optimal layer with the highest classification accuracy. We fine-tune the pre-trained models further by
applying self-supervised next token prediction loss in Eq 2, together with cross-entropy loss applied for probing
layers (with stop-gradients). We train the probing layers for 90 epochs, with a learning rate of6e−5. We also
use layer decay of 0.9 to reduce the learning rate at the early layers of the model. During this stage, all the
models are fine tuned with32 × 32 token resolution, on the self-supervised loss, and increase the base value of
the RoPE (Su et al., 2024) embeddings from 10,000 to 50,000 support larger resolution.
Table 7 shows the ImageNet top-1 accuracy of ourbase, largeand 1b models. First, there is a clear difference
in terms of classification performance when it comes to discriminative models versus generative models.
Instance discriminative models such as SimCLR (Chen et al., 2020b), and DINO (Caron et al., 2021) are
trained to separate samples from each other and they are designed to perform well on discriminative tasks. On
the other hand, generative models arejust trying to model the data distribution. While achieving comparable
performance to other generative models on image recognition, among autoregressive generative models, our
model achieved the highest top-1 accuracy. The scaling of data, and the use of tokens instead of pixels, allows
our one billion parameter model to achieve similar performance compared to iGPT (Chen et al., 2020a) 7
billion models.
Method Arch # θ Top1
Discriminative Approaches
SimCLR (Chen et al., 2020b)† RN50x2 94 74.2
BYOL (Grill et al., 2020)† RN50x2 94 77.4
SwAV (Caron et al., 2020)† RN50x2 94 73.5
DINO (Caron et al., 2021) ViT-B/8 86 80.1
DINOv2 (Oquab et al., 2023) ViT-g/14 1011 86.4
Generative Approaches
BEiT-L (Bao et al., 2021) ViT-L/14 307 62.2
AIM (El-Nouby et al., 2024) ViT-1B/14 1200 80.6
MAE (He et al., 2022) ViT-H/14 632 80.9
iGPT-L (Chen et al., 2020a)† GPT-2 1386 65.2
iGPT-XL (Chen et al., 2020a)† GPT-2 6801 72.0
Toto-base LLaMA 120 64.7
Toto-large LLaMA 280 71.1
Toto-1b LLaMA 1100 75.3
Table 7 ImageNet Results:We compare discriminative and generative models on ImageNet (Deng et al., 2009) recognition
task. While achieving comparable performance among generative models, our models model achieves the highest
accuracy on autoregressive modeling.†models are evaluated with linear probing.
4.3 Action Recognition
We use Kinetics-400 (K400) (Kay et al., 2017) for evaluating our models on action recognition tasks. Similar
to ImageNet evaluation, we apply a probe at each layer of the model, with attention pooling, and choose the
optimal layer with the highest action classification accuracy. We also fine-tune the pre-trained models on
a self-supervised next-patch prediction task while training the probing layers with a classification loss. All
our video models are trained with 16 frames, thus with a context length of 4096 tokens per video. When
evaluating videos, we follow the protocol in SlowFast (Feichtenhofer et al., 2019). Unlike ImageNet where we
evaluate the models at 256x256 resolution, on videos we only evaluate our models at 128x128 resolution, to
keep the number of tokens in a similar budget.
7Table 8 shows the Kinetics-400 top-1 accuracy of ourbase, largeand 1b models. Similar to ImageNet results
in Table 7, we see that discriminately trained models perform better than generative models. Our models
achieve comparable performance among generative models, and first to show competitive performance on
action recognition with autoregressive generative modeling. All the models are trained and evaluated with 16
frames with a stride of 4 frames.
Method Arch Top1
Discriminative Approaches
I-JEPA (Assran et al., 2023) ViT-H/16 74.5
OpenCLIP (Cherti et al., 2023) ViT-G/14 83.3
DINOv2 (Oquab et al., 2023) ViT-g/14 84.4
InternVideo (Wang et al., 2022) - 73.7
Generative Approaches
Hiera (Ryali et al., 2023) Hiera-H/14 77.0
MVD (Wang et al., 2023b) ViT-H/14 79.4
VideoMAE (Wang et al., 2023a) ViT-L/14 79.8
Toto-base LLaMA 59.3
Toto-large LLaMA 65.3
Toto-1b LLaMA 74.4
Table 8 K400 Results:We compare discriminative and generative models on Kinetics-400 (Kay et al., 2017) action
recognition task. While achieving comparable performance among generative models, our models are the first to show
the competitive performance on K400 with autoregressive pre-training, and shows scaling with large model sizes.
4.4 Action Forecasting
While the Kinetics dataset captures internet-style exocentric videos, Ego4D (Grauman et al., 2022) videos
capture day-to-day life egocentric videos. A general vision model should be able to reason about both exo
and ego-centric videos. Task-wise, Kinetics requires the model to reason about the action using full context
(e.g. the model has seen the action), while the Ego4D short-term action anticipation v1 task requires models
to predict future actions from past context. We use our models as the backbone for the pyramid network used
in StillFast (Ragusa et al., 2023) extract tokens at 5 layers and fuse them with the pyramid network. We
fully fine-tuned our model with self-supervised next-patch loss along with task-related losses, and we observed
having self-supervision loss improves overall performance. Table 9 shows the performance of ourlarge model
on the Ego4D short-term action anticipation task. This task requires predicting the object to be interacted
with (noun) and the type of interaction (verb) as well as time to contact (ttc) from the last seen frame to an
estimated time between object-hand contact. As shown in Table 9, these tasks are difficult with maximum
overall mean-average precision of 2.70.
Method Noun N+V N+TTC Overall
FRCNN+Rnd (Grauman et al., 2022) 17.55 1.56 3.21 0.34
FRCNN+SF (Grauman et al., 2022) 17.55 5.19 5.37 2.07
Hiera-large (Ryali et al., 2023) 14.05 6.03 4.53 2.12
StillFast (Ragusa et al., 2023) 16.20 7.47 4.94 2.48
VideoMAE-large (Wang et al., 2023a) 15.16 6.72 5.26 2.55
MAE-ST-large (Feichtenhofer et al., 2022) 13.71 6.63 4.94 2.60
Toto-large 15.20 6.75 5.41 2.70
Table 9 Ego4D Results: Our model achieves comparable mean-average precision compared to previous work. We
compare our method with, FRCNN+Rnd (Grauman et al., 2022), FRCNN+SF (Grauman et al., 2022), Hiera (Ryali
et al., 2023), StillFast (Ragusa et al., 2023), VideoMAE (Wang et al., 2023a), and MAE-ST (Feichtenhofer et al., 2022).
8Figure 5 Semi-Supervised Tracking:We follow the protocol in STC (Jabri et al., 2020), start with the GT segmentation
mask, and propagate the labels using the features computed byToto-large. The mask was propagated up to 60 frames
without losing much information.
(a) Franka Pick
 (b) Kuka Pick
 (c) Franka Cabinet
 (d) Kuka Cabinet
Figure 6 Robot Manipulation with Reinforcement Learning:We compare MAE-base (Radosavovic et al., 2022) with
Toto-base pre-trained models in simulation following Xiao et al. (2022). We evaluate each model the mean success rate
over training steps.Toto was able to learn these tasks faster than MAE, across two robots and two tasks.
4.5 Video Tracking
We study our pre-trained models on label propagation using the protocols in (Jabri et al., 2020) on DAVIS
dataset (Pont-Tuset et al., 2017). Compared to previous tasks such as classification, and forecasting, this
evaluation does not require finetuning or probing of the features. Following Jabri et al. (2020), we use the
features from the lastn frames to find the nearest neighbor patch in the current frame, and then propagate
the masks from the previous frames to the current frame.Comparison with Dino (Caron et al., 2021) and
MAE (He et al., 2022) is shown in Table 10 and qualitative results are shown in Figure 5.
Method (Res/Patch) J&F J F
DINO-base (224/8) 54.3 52.5 56.1
DINO-base (224/16) 33.1 36.2 30.1
MAE-base (224/16) 31.5 34.1 28.9
Toto-base (256/8) 42.0 41.2 43.1
Toto-large (256/8) 44.8 44.4 45.1
Toto-1b (256/8) 46.1 45.8 46.4
Toto-large (512/8) 62.4 59.2 65.6
Table 10 DAVIS Tracking:We report J, F, and J&F scores at the peak layers of each model. We achieves comparable
performance as DINO and at large resolution (512), it outperforms all methods.
9Figure 7 Real-world Deployment:We show an example episode of our policy performing the cube picking task on a
Franka robot in the real world. We useToto-base to run the robot at real time, despite being a small model,Toto was
able to achieve about 63% success rate in real world setting.
4.6 Robotics
In this section, we study the effectiveness of our pre-trained representations for robotic manipulation. We
consider tasks in both simulation and in the real world. Real world experiments needs to run at real time, there
for we only useToto-base models, in both setting. Despite being a small model,Toto-base can achieve better
performance in simulation and on-par performance to state-of-the-art robot models in real world experiments.
Simulation Experiments:Following the protocols in MVP (Xiao et al., 2022), we use our visual pre-trained
models to embed pixel observations. The model is frozen and we only take tokens at an intermediate layer,
apply average pooling, and learn the linear layer on top to embed pixel observations. These observations are
used to train DAgger policies for 4 different tasks: Franka-pick 6a, Kuka-pick 6b, Franka-cabinet 6c, and
Kuka-cabinet tasks 6d. Figure 6 shows the mean success rate over training steps. Compared to the MVP
baseline, our model was able to learn these tasks faster with better sample efficiency across robots and tasks.
For fair comparisons, we use the best MAE model from MVP (Radosavovic et al., 2022) which is trained on
ImageNet (Deng et al., 2009), Ego4D (Grauman et al., 2022) and 100DOH (Shan et al., 2020) datasets.
Model # Traj Success
MVP 240 75%
Toto-base 240 63%
Table 11 Robotics, Real-world Experiments:We compare
MVP (Radosavovic et al., 2022) andToto on a Franka
cube-picking task in the real world. Features from
both models are pre-trained, frozen, and passed into a
learning module trained with behavior cloning using
the same demonstrations. We see that our approach
performs comparably to the state-of-the-art vision
backbone for robotics, despite not being designed with
the robotic application in mind.
Real-world Experiments: Next, we evaluate our pre-
trained representations in the real world. We follow
the setup from (Radosavovic et al., 2022). We extract
vision features using a pre-trained vision encoder and
train a controller on top of frozen representations using
behavior cloning. Specifically, we consider a cube pick-
ing tasks using a 7 DoF Franka robot, shown in Figure 7.
We use the demonstrations provided by (Radosavovic
et al., 2023). In Table 11 we compare our model to
a vision encoder from (Radosavovic et al., 2022). We
report the success rate over 16 trials with variations in
object position and orientation. Our model performs
favorably to a vision encoder pre-trained for robotics.
4.7 Object Permanence
Method Model 16 32
V3D ResNet 55.2 69.7
TFC V3D ResNet 54.6 70.2
Toto-large LLaMa 62.8 72.9
Table 12 Object Permanence: CATER (Girdhar &
Ramanan, 2019) object localization task, where the
object is hidden under or obstructed by other objects.
The model is trained to predict its coarse location.
Our model performs better than previous methods on
snitch localization task at 16, 32 temporal resolutions.
To quantitatively measure the performance of how well
the model understands object permanence, we evaluate
our models on CATER localization task (Girdhar &
Ramanan, 2019). Here, a ball is moving in the scene,
and the task is to find its location in the 6 by 6 grid.
We fine tune ourToto-large model on this task at tem-
poral resolutions 16, and 32 frames. In both cases, our
pre-trained models were better at localizing the target
compared to models trained specifically for this task,
such as V3D (Zhang, 2022), TFC-V3D (Zhang, 2022).
Table??shows the performance on the CATER snitch lo-
calization task, andToto-large achieve 62.8% and 70.9%
performance with 16 and 32 frames respectively.
10Figure 8 Probing Across Layers, Models, and Tasks:We study the behavior of our models across multiple layers and tasks.
For image classification, action recognition, and object tracking, all the models behave similarly and peak around 50%
of the model depth. This behavior is observed across all model sizes. Robot tasks show a similar behaviour, where the
middle layers perform good at picking the objects, but last layers also perform good as middle layers. These plots
suggests, in decoder-only model, first half of the model starts to behave like an encoder, and compress the information,
and then rest of the model, projects the compressed semantic features back to input space.
4.8 Probing Across Layers
As shown in Figure 4 for the ImageNet classification task, different layers of the model contribute to the task
differently for the image classification task; this behavior is also observed in iGPT (Chen et al., 2020a). To
study this behavior across multiple tasks, we train probing layers for action recognition, object tracking, and
robot manipulation. Figure 8 shows probing performance across layers, model size, and tasks. It shows that
action recognition follows a similar trend to ImageNet classification, having peak performance at the middle
of the model stacks. While Object tracking also shares a similar trend with image classification and action
recognition, object manipulation shows an interesting trend of the last layers performing well as middle layers
from picking objects. Compared to the first three tasks, robot manipulation has a generative nature as a
task and can benefit from generative pre-training. In encoder models (Caron et al., 2021) or encoder-decoder
models (He et al., 2022; Bao et al., 2021) the last layer of the encoder has more semantic features.This may
suggest that, in decoder-only model, first half of the model starts to behave like an encoder, and compress the
information, and then rest of the model, projects the compressed semantic features back to input space.
4.9 Compute Optimal Scaling
Figure 9 Scaling Toto:We train multiple variants ofToto,
with increasing hidden size and depth, with optimal learning
rates. We plot the validation loss vs the compute spent on
training in MACs. This shows a clear scaling behavior with
optimal compute.
We study the scaling behaviors ofToto using µ-
Parameterization (Yang et al., 2022). First we train
various models, a1-a6, with linearly increasing hid-
den size and number of layers (Table 15). All mod-
els use the VQGAN tokenizer (Esser et al., 2020).
We then optimize the learning rate for these mod-
els, with µ-Parameterization (Yang et al., 2022).
Figure 11 shows optimal learning rate of2−7 for
all of the model widths. Once we find the opti-
mal learning rate, we train a1-a6 models on our
data mixture (Table 2). Figure 9 shows the loss
vs training compute ofToto models. This shows a
clear power law relationship between the compute
and validation loss. Based on these experiments
Toto shows a power law ofL(C) = 7.32 · C−0.0378.
For comparison, the GPT-3 power law relation-
ship (Brown, 2020) isL(C) = 2.57 ·C−0.048. While
these are not comparable directly, the scaling coef-
ficients indicate how much change in loss to expect
for extra added compute. This suggests that the
visual next token prediction models, such asToto,
scale but at a slower rate than language models.
115 Limitations
Our study suggests several important limitations and opportunities for future work. A significant limitation
stems from the use of internet videos, which, unlike carefully curated datasets, introduces challenges related
to data quality and diversity. This variance in data quality can impact model performance, especially when
compared to models trained on more curated datasets. Another limitation is the use of tokenizer, this makes
the learning not end-to-end, and the representation and generation quality is bounded by the quality of
the tokenizer, and with quantized vectors, the quality is very much limited, this needs further explorations
to build a universal visual tokenizer. Another fundamental limitation is training on videos for next token
prediction task. The added redundancy in video frames, can hurt quality of the learned representations. See
Appendix A.1 for more discussion on this topic. Additionally, our exploration of various design choices are
based on ImageNet classification. While it does transfer to most of the tasks we considered in this paper,
it may not be the optimal configuration for many other tasks. Furthermore, we have not yet fully assessed
our method’s effectiveness in dealing with dense prediction tasks, fine-grained recognition, or comprehending
complex temporal dynamics over extended time frames. These areas represent key opportunities for further
research, aiming to broaden the fruitfulness of autoregressive pre-trained models.
6 Conclusion
We empirically studied autoregressive pre-training from images and videos. We curated a large video dataset
and conducted a large-scale evaluation across a range of diverse tasks, including image recognition, video
classification, video forecasting, object tracking, object permanence, and robotic manipulation. We performed
extensive ablation studies to understand different design choices and compared auto regressive pre-training
from videos to strong baselines across different tasks. We found that, despite minimal inductive biases, our
approach achieves competitive performance across all tasks. Finally, we studied the scaling behavior of visual
next token prediction models, and showed it scales with compute, but at a slower rate than text based next
token prediction models.
7 Acknowledgments
We thank Andrea Madotto, Po-Yao (Bernie) Huang, and Shiry Ginosar for helpful discussions. We’re grateful
to Ronghang Hu and Xinlei Chen for their help with TPU setup and code bases. We also thank Baifeng Shi
for helping us with robots evaluations. We thank Valentin Gabeur and Neerja Thakkar for their valuable
feedback on the paper.
References
Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and
Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive architecture. InProceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15619–15629, 2023.
Fred Attneave. Some informational aspects of visual perception.Psychological review, 1954.
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers.arXiv preprint
arXiv:2106.08254, 2021.
Tom B Brown. Language models are few-shot learners.arXiv preprint arXiv:2005.14165, 2020.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.NeurIPS, 2020.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised
learning of visual features by contrasting cluster assignments.Advances in neural information processing systems,
33:9912–9924, 2020.
12Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.
Emerging properties in self-supervised vision transformers. InProceedings of the IEEE/CVF international conference
on computer vision, pp. 9650–9660, 2021.
Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-700 human action
dataset. arXiv preprint arXiv:1907.06987, 2019.
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative
pretraining from pixels. InInternational conference on machine learning, pp. 1691–1703. PMLR, 2020a.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning
of visual representations. InInternational conference on machine learning, pp. 1597–1607. PMLR, 2020b.
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph
Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2818–2829, 2023.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. 2019.
Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal Shankar,
Joshua M Susskind, and Armand Joulin. Scalable pre-training of large autoregressive image models.arXiv preprint
arXiv:2401.08541, 2024.
Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis. 2021 ieee.
In CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12868–12878, 2020.
Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data
filtering networks.arXiv preprint arXiv:2309.17425, 2023.
Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In
Proceedings of the IEEE/CVF international conference on computer vision, pp. 6202–6211, 2019.
Christoph Feichtenhofer, Yanghao Li, Kaiming He, et al. Masked autoencoders as spatiotemporal learners.Advances
in neural information processing systems, 35:35946–35958, 2022.
Rohit Girdhar and Deva Ramanan. Cater: A diagnostic dataset for compositional actions and temporal reasoning.
arXiv preprint arXiv:1910.04744, 2019.
Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson
Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18995–19012, 2022.
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,
Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach
to self-supervised learning.Advances in neural information processing systems, 33:21271–21284, 2020.
Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint
arXiv:2312.00752, 2023.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual
representation learning. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.
9729–9738, 2020.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are
scalable vision learners. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.
16000–16009, 2022.
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B
Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling.arXiv preprint
arXiv:2010.14701, 2020.
Allan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as a contrastive random walk.Advances in
neural information processing systems, 33:19545–19560, 2020.
13Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution.
In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,
Proceedings, Part II 14, pp. 694–711. Springer, 2016.
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim
Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset.arXiv preprint arXiv:1705.06950,
2017.
Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels
using a learned similarity metric. In Maria Florina Balcan and Kilian Q. Weinberger (eds.),Proceedings of The
33rd International Conference on Machine Learning, volume 48 ofProceedings of Machine Learning Research, pp.
1558–1566, New York, New York, USA, 20–22 Jun 2016. PMLR.
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A
framework for attention-based permutation-invariant neural networks. InProceedings of the 36th International
Conference on Machine Learning, volume 97 ofProceedings of Machine Learning Research, pp. 3744–3753. PMLR,
09–15 Jun 2019.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network.arXiv preprint arXiv:1312.4400, 2013.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization.arXiv preprint arXiv:1711.05101, 2017.
Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m:
Learning a text-video embedding by watching hundred million narrated video clips. InProceedings of the IEEE/CVF
international conference on computer vision, pp. 2630–2640, 2019.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda
Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads.arXiv preprint arXiv:2209.11895,
2022.
Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without
supervision. arXiv preprint arXiv:2304.07193, 2023.
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image
transformer. In International conference on machine learning, pp. 4055–4064. PMLR, 2018.
Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017
davis challenge on video object segmentation.arXiv preprint arXiv:1704.00675, 2017.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative
pre-training. 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are
unsupervised multitask learners. 2019.
Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world robot
learning with masked visual pre-training. InConference on Robot Learning, 2022.
Ilija Radosavovic, Baifeng Shi, Letian Fu, Ken Goldberg, Trevor Darrell, and Jitendra Malik. Robot learning with
sensorimotor pre-training. InConference on Robot Learning, 2023.
Francesco Ragusa, Giovanni Maria Farinella, and Antonino Furnari. Stillfast: An end-to-end approach for short-term
object interaction anticipation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 3635–3644, 2023.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. InInternational Conference on Machine Learning, pp. 8821–8831. PMLR, 2021.
MarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan Collobert, and Sumit Chopra. Video
(language) modeling: a baseline for generative models of natural videos.arXiv preprint arXiv:1412.6604, 2014.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. InProceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pp. 10684–10695, 2022.
14Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.International journal of
computer vision, 115:211–252, 2015.
Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu
Chowdhury, Omid Poursaeed, Judy Hoffman, et al. Hiera: A hierarchical vision transformer without the bells-and-
whistles. arXiv preprint arXiv:2306.00989, 2023.
Dandan Shan, Jiaqi Geng, Michelle Shu, and David F Fouhey. Understanding human hands in contact at internet
scale. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9869–9878, 2020.
Claude E Shannon. Prediction and entropy of printed english.Bell system technical journal, 1951.
Noam Shazeer. Glu variants improve transformer.arXiv preprint arXiv:2002.05202, 2020.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.arXiv
preprint arXiv:1409.1556, 2014.
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer
with rotary position embedding.Neurocomputing, 568:127063, 2024.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste
Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971, 2023.
Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image
generation with pixelcnn decoders.Advances in neural information processing systems, 29, 2016.
Aäron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. InInternational
conference on machine learning, pp. 1747–1756. PMLR, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need.Advances in neural information processing systems, 30, 2017.
Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2:
Scaling video masked autoencoders with dual masking. InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 14549–14560, 2023a.
Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Lu Yuan, and Yu-Gang Jiang.
Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning. In
CVPR, 2023b.
Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun
Wang, et al. Internvideo: General video foundation models via generative and discriminative learning.arXiv preprint
arXiv:2212.03191, 2022.
Dirk Weissenborn, Oscar Täckström, and Jakob Uszkoreit. Scaling autoregressive video models.arXiv preprint
arXiv:1906.02634, 2019.
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance
discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3733–3742,
2018.
Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for motor control.arXiv
preprint arXiv:2203.06173, 2022.
Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki,
Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter
transfer, 2022.
Biao Zhang and Rico Sennrich. Root mean square layer normalization.Advances in Neural Information Processing
Systems, 32, 2019.
Shiwen Zhang. Tfcnet: Temporal fully connected networks for static unbiased temporal reasoning.arXiv preprint
arXiv:2203.05928, 2022.
Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient
visual representation learning with bidirectional state space model.arXiv preprint arXiv:2401.09417, 2024.
15A Appendix
A.1 Video Tokens for Pre-Training
The next patch prediction for visual pre-training is equivalent to the next token prediction in large language
models. However, most languages have a clear sequential nature, therefore there is a clear definition for the
next word. This also makes the next word prediction task relatively harder, since the model requires learning
to extrapolate the data. On the other hand, images and videos, especially over the spatial dimensions lack a
sequential nature. We follow the previous works (Chen et al., 2020a; Van Den Oord et al., 2016) to make the
images and videos into a 1D sequence by scanning the patches in raster order. While this ordering allows for
example to learn to predict the bottom half of the image from the top part of the image, in many places, the
tokens can be predicted by interpolating rather than extrapolating.
On the time axis, yes, there is a clear sequential nature, however, video frames compared to text tokens are
more redundant, making the next frame prediction task much easier. Figure 10 shows average validation loss
over 4096 token, in kinetics 400 dataset (Kay et al., 2017), onToto-large model. This shows there is high loss
of the first frame, but the subsequent frames have relatively lower loss compared to the first frame. This is
because, even with reasonably lower sampling rate, frames following the first frame has some redundancy,
and hinders the learning, since these tokens are relatively easy to predict. This also could be attributed by
emergence of induction heads (Olsson et al., 2022). While we focused on learning from unfiltered internet
scale video with minimal inductive bias, to learn efficiently from videos, need further research in this direction.
Figure 10 Average Validation Loss Over Tokens:We show the average loss per token for kinetics validation set. It clearly
shows the redundancy in videos, as the first frame has higher prediction loss, and rest of the frames on average has
lower loss than the first frame.
A.2 Prefix attention
During fine-tuning, we experimented with causal and full attention. On ImageNet, our base model achieved
full attn: 82.6% vs causal attn: 82.2%. Even though our models arenot pre-trained with prefix attention, still
able to utilize full attn at fine-tuning. This is an unrealized benefit of training with videos, (a middle token in
say, 8th frame won’t see the rest half of the 8th frame, but have seen all the tokens from 7th frame, which are
similar because of video, hence approximating full attention at pre-training)
A.3 Full fine-tuning
We fine-tuned our models on ImageNet, and performance is close to SOTA, compared to linear probing (where
we only use causal attention). But during the fine-tuning, we use full attention.
DINO MoCo v3 BEiT MAE Toto
82.8 83.2 83.2 83.6 82.6
Table 13 Full Fine Tuning Performance:Comparison of different methods performance on ImageNet-1K.
16A.4 iGPT vs Totoon ImagenNet
Table 7 shows ImageNet evaluation performance. However, iGPT (Chen et al., 2020a) models are evaluated
only using linear probing. To have a fair comparison, between iGPT andToto, we reevaluated our models
using linear probing. Both models have causal attention and are trained on auto-regressive objectives. On the
same model sizes, about 1 billion parameters, our achieve 66.2% while the similar iGPT model’s ImageNet
performance is 65.2%. This fair evaluation suggests the modifications made onToto have clear benefits over
iGPT.
Method Arch # θ Top1
iGPT-L (Chen et al., 2020a) GPT-2 1386 65.2
Toto-1b LLaMA 1100 66.2
Table 14 ImageNet Linear Probing Results:Toto performs better than similar size iGPT models.
A.5 µ-Parameterization
To study the scaling behaviours ofToto using µ-Parameterization (Yang et al., 2022). First we train various
models a1-a6 (in Table 15), with hidden sizes (64-1536) and number of layers (12-48), increasing linearly and
we used VQGAN tokenizer (Esser et al., 2020). Then we tune the learning rate for these models, with fixed
depth usingµ-Parameterization (Yang et al., 2022). Figure 11 shows optimal learning rate of2−7 for all the
model widths. Once we find the optimal learning rate, we train a1-a6 models on the mixture of image and
video data, as mentioned in Table 2.
Model Params Dimension Heads Layers
a1 14.8M 256 16 12
a2 77.2M 512 16 16
a3 215M 768 16 20
a4 458M 1024 16 24
a5 1.2B 1536 16 28
a6 1.9B 1792 16 32
Table 15 Toto Varients:We scaleTotomodels by increasing
hidden dimension and number of layers linearly while
keeping number of heads constant following (Yang et al.,
2022; Touvron et al., 2023).
Figure 11 µ-Parameterization Learning Rate:We show that
µ-Parameterization (Yang et al., 2022), we can train all
width Toto models, with an single optimal learning rate
of 2−7.
17A.6 n-gram distribution
In this section, we compare the 2-gram and 3-gram distribution of dVAE (Ramesh et al., 2021), VQGAN (Esser
et al., 2020) image tokeizers. We compute 2-gram and 3-gram distributions on the discrete tokens of 10000
ImageNet validation images. Figure 12 and Figure 13 show the distributions of these tokenizers respectively.
On 2-gram distribution, dVAE (Ramesh et al., 2021) has more discrete combination of tokens compared to
both VQGAN-1K and VQGAN-16k tokenizers.
Figure 12 2-gram Distribution of Various Tokens:We compute the 2-gram distribution on 10000 images from the ImageNet
validation set. Compared to VQGAN 1k and 16k vocabulary tokenizers, the dVAE tokenizer has a larger set of token
combinations.
Figure 13 3-gram Distribution of Various Tokens:We compute the 3-gram distribution on 10000 images from the ImageNet
validation set. All the tokenizers has similar almost flat distribution when it comes to 3-gram tokens.
A.7 Attention probing variants on K400
We also evaluate our models and baselines on the Kinetics 400 dataset using a variant of attention probing. In
the main paper, we use attention probing, with only learningWk, Wv matrices, and a single learnable query
vector. We also test with cross attention with MLP layers as the attention classifier, to give more capacity to
the learnable head. Table 16 show the performance on the attention classifier with an additional MLP head.
This helps to improve performance across over all models.
Method Arch Top1
Hiera (Ryali et al., 2023) Hiera-L/14 74.2
Hiera (Ryali et al., 2023) Hiera-H/14 75.2
VideoMAE (Wang et al., 2023a) ViT-B/14 65.4
VideoMAE (Wang et al., 2023a) ViT-L/14 74.8
Toto-base LLaMA 61.2
Toto-large LLaMA 65.8
Toto-1b LLaMA 74.8
Table 16 K400 Results:We evaluate our models using cross attention and MLP layer as the classification head. Overall
using a high-capacity head improves the performance across all models.
18A.8 Generation samples
long video generation:we can generate up to 64 frames, first raw: periodic motion, second raw: object
permanence (light stand).
prompting (pre-trained model):shows 3D rotation
prompting (finetuned model):A small 1000-step fine-tuning leads to a promptable model for various
vision tasks.
A.9 Additional Layer-wise Probing Results
We probe the multiple variants of our models at each layer for the best ImageNet performance. First, we test
the models on linear probing, on both sizes of 128 and 256 resolution. Figure 14 presents the probing curves of
the models trained with attention probing at 128 resolution. Across all models, the performance has a similar
behavior to the pre-trained models, with peak performance around the middle of the depth of the model.
Figure 14 Training Loss Curves:We show the training loss curves for multiple variants of our models.
19